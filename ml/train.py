import torch
import torch.nn as nn
import pandas as pd
import numpy as np
import pandas_ta as ta
from sklearn.preprocessing import MinMaxScaler
import sys
import os
import json

# Configuration
base_dir = os.path.dirname(os.path.abspath(__file__))
# Default to QQQ 5min in ml/data/
default_data = os.path.join(base_dir, 'data', 'QQQ_5min.csv')
input_file = sys.argv[1] if len(sys.argv) > 1 else default_data

model_file = os.path.join(base_dir, 'model.pth')
scaler_file = os.path.join(base_dir, 'scaler.json')

lookback = 60
hidden_size = 64 # Slightly larger
num_epochs = 100
learning_rate = 0.001

print(f"Loading data from {input_file}...")

if not os.path.exists(input_file):
    print(f"Error: {input_file} not found.")
    sys.exit(1)

df = pd.read_csv(input_file)

# --- Feature Engineering (Quant Upgrade) ---
# Calculate Indicators using pandas_ta
# RSI
df.ta.rsi(length=14, append=True)
# MACD (12, 26, 9)
df.ta.macd(fast=12, slow=26, signal=9, append=True)
# Bollinger Bands? Maybe later.
# Volume ?
# Let's normalize Volume.

# Drop NaNs created by indicators
df.dropna(inplace=True)

# Select Features
# Close, RSI_14, MACDh_12_26_9 (Histogram), volume
# Note: 'MACDh_12_26_9' is the histogram column name usually generated by pandas_ta
feature_cols = ['close', 'RSI_14', 'MACDh_12_26_9', 'volume']
target_col = 'close'

print(f"Features: {feature_cols}")

data_features = df[feature_cols].values.astype(float)
data_target = df[[target_col]].values.astype(float)

# Normalize Features
scaler = MinMaxScaler()
data_features_scaled = scaler.fit_transform(data_features)

# Separate scaler for Target (to inverse transform prediction)
target_scaler = MinMaxScaler()
# Transform target to 0-1 range!
data_target_scaled = target_scaler.fit_transform(data_target)

# Prepare Data for LSTM
def create_dataset(features, target, lookback):
    X, y = [], []
    for i in range(len(features) - lookback - 1):
        feature_seq = features[i : i + lookback]
        target_val = target[i + lookback + 1] # Next close
        X.append(feature_seq)
        y.append(target_val)
    return np.array(X), np.array(y)

X, y = create_dataset(data_features_scaled, data_target_scaled, lookback)

X_train = torch.from_numpy(X).float()
y_train = torch.from_numpy(y).float()

input_size = X_train.shape[2] # Number of features
print(f"Input Size (Features): {input_size}")

# LSTM Model Definition
class QuantPredictor(nn.Module):
    def __init__(self, input_size, hidden_size, output_size=1):
        super(QuantPredictor, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True, num_layers=2, dropout=0.2)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        out, _ = self.lstm(x)
        out = self.fc(out[:, -1, :]) 
        return out

model = QuantPredictor(input_size=input_size, hidden_size=hidden_size)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

print("Training Quant model...")
for epoch in range(num_epochs):
    model.train()
    outputs = model(X_train)
    # y_train is (Samples, 1). output is (Samples, 1)
    # However y comes from create_dataset as (Samples, 1) ? No, target_val matches target shape.
    # We need to scale y_train properly. Yes we did.
    
    # We need to reshape y_train to match outputs?
    # y_train shape is likely (N, 1) if data_target was (M, 1).
    loss = criterion(outputs, y_train)
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if (epoch+1) % 10 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.6f}')

print("Training complete.")

# Save Model
torch.save(model.state_dict(), model_file)
print(f"Model saved to {model_file}")

# Save scaler params
# We need to save params/scaler to be able to scale NEW input the EXACT same way.
# Since we have multiple features, we should pickle the scaler object using joblib?
# Or for portability, just verify we can load it.
# Simple JSON won't work easily for multivariate scaler.
# Let's use joblib since we installed scikit-learn (which has joblib dependency usually)
import joblib
joblib.dump(scaler, os.path.join(base_dir, 'scaler_features.save'))
joblib.dump(target_scaler, os.path.join(base_dir, 'scaler_target.save'))
print("Scalers saved via joblib.")
